{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Checklist Feedback\n","\n","This notebook contains the evaluation prompt used to assess the checklist responses and provide feedback."]},{"cell_type":"markdown","metadata":{},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:58:34.265693Z","iopub.status.busy":"2024-10-15T15:58:34.265307Z","iopub.status.idle":"2024-10-15T15:58:49.076803Z","shell.execute_reply":"2024-10-15T15:58:49.075558Z","shell.execute_reply.started":"2024-10-15T15:58:34.265653Z"},"trusted":true},"outputs":[],"source":["%pip install -qU openai"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import threading\n","import pickle\n","import shutil\n","import json\n","import time  \n","import re\n","\n","from openai import AzureOpenAI\n","\n","KEY1 = \"XXX\"\n","KEY2 = \"XXX\"\n","API_VERSION = \"XXX\"\n","LOCATION = \"XXX\"\n","ENDPOINT = \"XXX\"\n","DEPLOYMENT = \"XXX\"\n","\n","client = AzureOpenAI(\n","  azure_endpoint=ENDPOINT,\n","  api_key=KEY2,\n","  api_version=API_VERSION\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Prompt for evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T15:58:50.320717Z","iopub.status.busy":"2024-10-15T15:58:50.320278Z","iopub.status.idle":"2024-10-15T15:58:50.326298Z","shell.execute_reply":"2024-10-15T15:58:50.325249Z","shell.execute_reply.started":"2024-10-15T15:58:50.320681Z"},"trusted":true},"outputs":[],"source":["prompt_template = \"\"\"You are provided with a \"Paper\" to be submitted to the NeurIPS conference. You are assisting the authors in preparing their \"Answer\" to one checklist \"Question\". Please examine carefully the proposed author's \"Answer\" and the proposed author's \"Justification\" provided, and identify any discrepancies with the actual \"Paper\" content, for this specific \"Question\", taking into account the \"Guidelines\" provided to authors.\n","\n","Afterwards, provide itemized, actionable feedback, based on the \"Guidelines\", aiming to improve the paper quality. Concentrate on a few of the most significant improvements that can be made, and write in terse technical English. While Authors' Proposed Answer is generally preferred to be a \"Yes\", it is acceptable to answer \"No\" or \"NA\" provided a proper Authors' Proposed Justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). If the Authors' Proposed Answer is Yes, the Authors' Proposed Justification for the Answer should point to the section(s) within which related material for the question can be found. Note that the Authors' Proposed Justification is not expected to contain anything else (although it is fine if it contains more details).\n","\n","Finally, after performing all previous steps, conclude your review with a score for this specific \"Question\", in a separate line (1: Everything OK or mild issues; 0.5: Needs improvements. Use this score sparingly; 0: Critical issues). Make sure that score is shown in a new line in this format \"Score: score_value\" and there is no content after the score.\n","\n","Question:\n","<START OF QUESTION>\n","{question}\n","<END OF QUESTION>\n","\n","Answer:\n","<START OF ANSWER>\n","{answer}\n","<END OF ANSWER>\n","\n","Justification:\n","{justification}\n","\n","Guidelines:\n","<START OF GUIDELINES>\n","{guideline}\n","<END OF GUIDELINES>\n","\n","Paper:\n","<START OF PAPER>\n","{paper}\n","<END OF PAPER>\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Process evaluation for all papers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-02T04:22:28.475720Z","iopub.status.busy":"2024-10-02T04:22:28.475304Z","iopub.status.idle":"2024-10-02T04:34:22.125269Z","shell.execute_reply":"2024-10-02T04:34:22.124101Z","shell.execute_reply.started":"2024-10-02T04:22:28.475681Z"},"trusted":true},"outputs":[],"source":["def process_submission_chunk(chunk):\n","    for _, row in chunk.iterrows():\n","        sub_id = row['submission_id'] \n","        df_qa = pd.read_csv(f'submissions/{sub_id}/paper_checklist.csv')\n","        with open(f'submissions/{sub_id}/article_dict.pickle', 'rb') as f:\n","            dict_paper = pickle.load(f)\n","\n","        str_sections = f\"Abstract:\\n{dict_paper['abstract']}\\n\\n\"\n","        for sec in dict_paper['sections']:\n","            if sec['heading'] in ['Claims', 'Limitations']:\n","                break\n","            str_sections += f\"Section {sec['heading']}:\\n{sec['text']}\\n\\n\"\n","\n","        # run the evaluation 3 times\n","        for run in range(3):  \n","            score_reproduce = []\n","            \n","            # run the evaluation for 15 questions\n","            for id_q in range(15):\n","                msg = prompt_template.format(\n","                    question=df_qa['Question'][id_q],\n","                    answer=df_qa['Answer'][id_q],\n","                    justification=df_qa['Justification'][id_q],\n","                    guideline=df_qa['Guidelines'][id_q],\n","                    paper=str_sections,\n","                )\n","\n","                response = client.chat.completions.create(\n","                    model=DEPLOYMENT,\n","                    messages=[\n","                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","                        {\"role\": \"user\", \"content\": msg},\n","                    ]\n","                )\n","                try:\n","                    output = response.choices[0].message.content\n","                    # Extract the score (0, 0.5, 1)\n","                    matches = re.findall(r'\\b(0\\.5|1|0)\\b', output[-450:])\n","                    float_matches = [float(match) for match in matches][0]\n","\n","                    score_reproduce.append(float_matches)\n","                except:\n","                    score_reproduce.append(0)\n","\n","            df_qa[f'run-{run}'] = score_reproduce\n","\n","        df_qa.to_csv(f'submissions/{sub_id}/paper_checklist.csv', index=False)\n","        print(sub_id, \" saved!\")\n","            \n","def split_dataframe(df, num_chunks):\n","    chunk_size = len(df) // num_chunks\n","    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n","    return chunks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_sub_all = pd.read_csv('XXX')\n","num_threads = 2 # num of threads for faster computation\n","\n","chunks = split_dataframe(df_sub_all, num_threads)\n","\n","threads = []\n","for i in range(num_threads):\n","    thread = threading.Thread(target=process_submission_chunk, args=(chunks[i],))\n","    threads.append(thread)\n","    thread.start()\n","\n","for thread in threads:\n","    thread.join()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5666839,"sourceId":9349067,"sourceType":"datasetVersion"},{"datasetId":5807259,"sourceId":9534830,"sourceType":"datasetVersion"},{"datasetId":5881310,"sourceId":9633249,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
